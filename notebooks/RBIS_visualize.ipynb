{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import fiona\n",
    "import h5py\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gdal\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "%matplotlib widget\n",
    "#import earthpy.Spatial as es\n",
    "\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# File name format: ATL06_[yyyymmdd][hhmmss]_[RGTccss]_[vvv_rr].h5\n",
    "\n",
    "#NOTE: Need to simplify this function\n",
    "def time_from_fname(fname):\n",
    "    \"\"\" IS2 fname -> datatime object. \"\"\"\n",
    "    t = fname.split('_')[1]\n",
    "    y, m , d, h, mn, s = t[:4], t[4:6], t[6:8], t[8:10], t[10:12], t[12:14]\n",
    "    time = dt.datetime(int(y), int(m), int(d), int(h), int(mn), int(s))\n",
    "    return time\n",
    "\n",
    "\n",
    "def segment_from_fname(fname):\n",
    "    \"\"\" IS2 fname -> segment number. \"\"\"\n",
    "    s = fname.split('_')[2]\n",
    "    return int(s[-2:])\n",
    "\n",
    "\n",
    "def select_files(files, segments=[10,11,12], t1=(2019,1,1), t2=(2019,2,1)):\n",
    "    t1 = dt.datetime(*t1)\n",
    "    t2 = dt.datetime(*t2)\n",
    "    files_out = []\n",
    "    for f in files:\n",
    "        fname = os.path.basename(f)\n",
    "        time = time_from_fname(fname)\n",
    "        segment = segment_from_fname(fname)\n",
    "        if t1 <= time <= t2 and segment in segments:\n",
    "            files_out.append(f)\n",
    "    return files_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from astropy.time import Time\n",
    "\n",
    "def gps2dyr(time):\n",
    "    \"\"\" Converte GPS time to decimal years. \"\"\"\n",
    "    return Time(time, format='gps').decimalyear\n",
    "\n",
    "\n",
    "def track_type(time, lat, tmax=1):\n",
    "    \"\"\"\n",
    "    Separate tracks into ascending and descending.\n",
    "    \n",
    "    Defines tracks as segments with time breaks > tmax,\n",
    "    and tests whether lat increases or decreases w/time.\n",
    "    \"\"\"\n",
    "    tracks = np.zeros(lat.shape)  # generate track segment\n",
    "    tracks[0:np.argmax(np.abs(lat))] = 1  # set values for segment\n",
    "    i_asc = np.zeros(tracks.shape, dtype=bool)  # output index array\n",
    "\n",
    "    # Loop trough individual secments\n",
    "    for track in np.unique(tracks):\n",
    "    \n",
    "        i_track, = np.where(track == tracks)  # get all pts from seg\n",
    "    \n",
    "        if len(i_track) < 2: continue\n",
    "    \n",
    "        # Test if lat increases (asc) or decreases (des) w/time\n",
    "        i_min = time[i_track].argmin()\n",
    "        i_max = time[i_track].argmax()\n",
    "        lat_diff = lat[i_track][i_max] - lat[i_track][i_min]\n",
    "    \n",
    "        # Determine track type\n",
    "        if lat_diff > 0:  i_asc[i_track] = True\n",
    "    \n",
    "    return i_asc, np.invert(i_asc)  # index vectors\n",
    "\n",
    "\n",
    "def transform_coord(proj1, proj2, x, y):\n",
    "    \"\"\"\n",
    "    Transform coordinates from proj1 to proj2 (EPSG num).\n",
    "\n",
    "    Example EPSG projs:\n",
    "        Geodetic (lon/lat): 4326\n",
    "        Polar Stereo AnIS (x/y): 3031\n",
    "        Polar Stereo GrIS (x/y): 3413\n",
    "    \"\"\"\n",
    "    # Set full EPSG projection strings\n",
    "    proj1 = pyproj.Proj(\"+init=EPSG:\"+str(proj1))\n",
    "    proj2 = pyproj.Proj(\"+init=EPSG:\"+str(proj2))\n",
    "    return pyproj.transform(proj1, proj2, x, y)  # convert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def read_atl06(fname, bbox=None):\n",
    "    \"\"\" \n",
    "    Read 1 ATL06 file and output 6 reduced files. \n",
    "    \n",
    "    Extract variables of interest and separate the ATL06 file \n",
    "    into each beam (ground track) and ascending/descending orbits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each beam is a group\n",
    "    group = ['/gt1l', '/gt1r', '/gt2l', '/gt2r', '/gt3l', '/gt3r']\n",
    "\n",
    "    # Loop trough beams\n",
    "    for k,g in enumerate(group):\n",
    "    \n",
    "        #-----------------------------------#\n",
    "        # 1) Read in data for a single beam #\n",
    "        #-----------------------------------#\n",
    "    \n",
    "        # Load variables into memory (more can be added!)\n",
    "        with h5py.File(fname, 'r') as fi:\n",
    "            lat = fi[g+'/land_ice_segments/latitude'][:]\n",
    "            lon = fi[g+'/land_ice_segments/longitude'][:]\n",
    "            h_li = fi[g+'/land_ice_segments/h_li'][:]\n",
    "            s_li = fi[g+'/land_ice_segments/h_li_sigma'][:]\n",
    "            t_dt = fi[g+'/land_ice_segments/delta_time'][:]\n",
    "            q_flag = fi[g+'/land_ice_segments/atl06_quality_summary'][:]\n",
    "            s_fg = fi[g+'/land_ice_segments/fit_statistics/signal_selection_source'][:]\n",
    "            snr = fi[g+'/land_ice_segments/fit_statistics/snr_significance'][:]\n",
    "            h_rb = fi[g+'/land_ice_segments/fit_statistics/h_robust_sprd'][:]\n",
    "            dac = fi[g+'/land_ice_segments/geophysical/dac'][:]\n",
    "            f_sn = fi[g+'/land_ice_segments/geophysical/bsnow_conf'][:]\n",
    "            dh_fit_dx = fi[g+'/land_ice_segments/fit_statistics/dh_fit_dx'][:]\n",
    "            tide_earth = fi[g+'/land_ice_segments/geophysical/tide_earth'][:]\n",
    "            tide_load = fi[g+'/land_ice_segments/geophysical/tide_load'][:]\n",
    "            tide_ocean = fi[g+'/land_ice_segments/geophysical/tide_ocean'][:]\n",
    "            tide_pole = fi[g+'/land_ice_segments/geophysical/tide_pole'][:]\n",
    "            t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:]\n",
    "            rgt = fi['/orbit_info/rgt'][:] * np.ones(len(lat))\n",
    "            orb = np.full_like(h_li, k)\n",
    "\n",
    "        #---------------------------------------------#\n",
    "        # 2) Filter data according region and quality #\n",
    "        #---------------------------------------------#\n",
    "        \n",
    "        # Select a region of interest\n",
    "        if bbox:\n",
    "            lonmin, lonmax, latmin, latmax = bbox\n",
    "            bbox_mask = (lon >= lonmin) & (lon <= lonmax) & \\\n",
    "                        (lat >= latmin) & (lat <= latmax)\n",
    "        else:\n",
    "            bbox_mask = np.ones_like(lat, dtype=bool)  # get all\n",
    "            \n",
    "        # Only keep good data, and data inside bbox\n",
    "        mask = (q_flag == 0) & (np.abs(h_li) < 10e3) & (bbox_mask == 1)\n",
    "        \n",
    "        # Update variables\n",
    "        lat, lon, h_li, s_li, t_dt, h_rb, s_fg, snr, q_flag, f_sn, \\\n",
    "            tide_earth, tide_load, tide_ocean, tide_pole, dac, rgt, orb = \\\n",
    "                lat[mask], lon[mask], h_li[mask], s_li[mask], t_dt[mask], \\\n",
    "                h_rb[mask], s_fg[mask], snr[mask], q_flag[mask], f_sn[mask], \\\n",
    "                tide_earth[mask], tide_load[mask], tide_ocean[mask], \\\n",
    "                tide_pole[mask], dac[mask], rgt[mask], orb[mask]\n",
    "\n",
    "        # Test for no data\n",
    "        if len(h_li) == 0: continue\n",
    "\n",
    "        #-------------------------------------#\n",
    "        # 3) Convert time and separate tracks #\n",
    "        #-------------------------------------#\n",
    "        \n",
    "        # Time in GPS seconds (secs sinde 1980...)\n",
    "        t_gps = t_ref + t_dt\n",
    "\n",
    "        # Time in decimal years\n",
    "        t_year = gps2dyr(t_gps)\n",
    "\n",
    "        # Determine orbit type\n",
    "        i_asc, i_des = track_type(t_year, lat)\n",
    "        \n",
    "        #-----------------------#\n",
    "        # 4) Save selected data #\n",
    "        #-----------------------#\n",
    "        \n",
    "        # Define output file name\n",
    "        ofile = fname.replace('.h5', '_'+g[1:]+'.h5')\n",
    "                \n",
    "        # Save variables\n",
    "        with h5py.File(ofile, 'w') as f:\n",
    "            f['orbit'] = orb\n",
    "            f['lon'] = lon\n",
    "            f['lat'] = lat\n",
    "            f['h_elv'] = h_li\n",
    "            f['t_year'] = t_year\n",
    "            f['t_sec'] = t_gps\n",
    "            f['s_elv'] = s_li\n",
    "            f['h_rb'] = h_rb\n",
    "            f['s_fg'] = s_fg\n",
    "            f['snr'] = snr\n",
    "            f['q_flg'] = q_flag\n",
    "            f['f_sn'] = f_sn\n",
    "            f['tide_load'] = tide_load\n",
    "            f['tide_ocean'] = tide_ocean\n",
    "            f['tide_pole'] = tide_pole\n",
    "            f['tide_earth'] = tide_earth\n",
    "            f['dac'] = dac\n",
    "            f['rgt'] = rgt\n",
    "            f['trk_type'] = i_asc\n",
    "\n",
    "            print('out ->', ofile)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def read_atl03(fname, bbox=None):\n",
    "    print(fname)\n",
    "    \"\"\" \n",
    "    Read 1 ATL06 file and output 6 reduced files. \n",
    "    \n",
    "    Extract variables of interest and separate the ATL06 file \n",
    "    into each beam (ground track) and ascending/descending orbits.\n",
    "    \"\"\"\n",
    "    f = h5py.File(fname,'r')\n",
    "    # Each beam is a group\n",
    "    group = ['/gt1l', '/gt1r', '/gt2l', '/gt2r', '/gt3l', '/gt3r']\n",
    "    # Loop trough beams\n",
    "    for k,g in enumerate(group):\n",
    "    \n",
    "        #-----------------------------------#\n",
    "        # 1) Read in data for a single beam #\n",
    "        #-----------------------------------#\n",
    "    \n",
    "        # Load variables into memory (more can be added!)\n",
    "        with h5py.File(fname, 'r') as fi:\n",
    "            if g+'/heights' in fi.keys():\n",
    "                lat = fi[g+'/heights/lat_ph'][:]\n",
    "                lon = fi[g+'/heights/lon_ph'][:]\n",
    "                h = fi[g+'/heights/h_ph'][:]\n",
    "                conf = fi[g+'/heights/signal_conf_ph']\n",
    "                bkg = fi[g+'/bckgrd_atlas/bckgrd_counts'][:]\n",
    "                dist = fi[g+'/heights/dist_ph_along'][:]\n",
    "                seg = fi[g+'/geolocation/segment_dist_x'][:]\n",
    "\n",
    "                land_ice_class = conf[:,3]\n",
    "          \n",
    "                #x_atc = dist+seg\n",
    "        #-----------------------------------#\n",
    "        # 3) Filter data #\n",
    "        #-----------------------------------#\n",
    "                mask = (land_ice_class == 4) & (np.abs(h) < 10e3)\n",
    "                lat,lon,h,x_atc = lat[mask],lon[mask],h[mask],x_atc[mask]\n",
    "        \n",
    "        #-----------------------#\n",
    "        # 4) Save selected data #\n",
    "        #-----------------------#\n",
    "        \n",
    "        # Define output file name\n",
    "                ofile = fname.replace('.h5', '_'+g[1:]+'.h5')\n",
    "                \n",
    "        # Save variables\n",
    "                with h5py.File(ofile, 'w') as f:\n",
    "                    f['lon'] = lon\n",
    "                    f['lat'] = lat\n",
    "                    f['h_elv'] = h\n",
    "                    f['bkc_ct'] = bkg\n",
    "                    f['x_atc'] = x_atc\n",
    "            \n",
    "                    print('out ->', ofile)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "\n",
    "def list_files_local(path):\n",
    "    \"\"\" Get file list form local folder. \"\"\"\n",
    "    from glob import glob\n",
    "    return glob(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pwd\n",
    "# files = list_files_local('../data/RBIS/*ATL03*01.h5')\n",
    "\n",
    "\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20181202114602_09900112_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20181230213426_00370210_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20190111210924_02200210_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20181231102210_00450212_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20181214112102_11730112_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20190128201036_04790210_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20190112095707_02280212_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20181213223319_11650110_001_01.h5')]\n",
    "# del files[files.index('../data/RBIS/processed_ATL03_20190214082459_07310212_001_01.h5')]\n",
    "\n",
    "# njobs = 1#len(files)\n",
    "\n",
    "# #NOTE: Using Kamb bounding box for now\n",
    "# bbox = None #[-1124782, 81623, -919821, -96334]\n",
    "\n",
    "# if njobs == 1:\n",
    "#     print('running in serial ...')\n",
    "#     [read_atl03(f, bbox) for f in files]\n",
    "\n",
    "# else:\n",
    "#     print('running in parallel (%d jobs) ...' % njobs)\n",
    "#     from joblib import Parallel, delayed\n",
    "#     Parallel(n_jobs=njobs, verbose=5)(delayed(read_atl06)(f, bbox) for f in files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in serial ...\n",
      "../data/RLIS/processed_ATL03_20181224114725_13260112_001_01.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (678564,) (1259,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-63ad13b9fef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnjobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running in serial ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mread_atl03\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-63ad13b9fef3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnjobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running in serial ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mread_atl03\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-e1ced15f3350>\u001b[0m in \u001b[0;36mread_atl03\u001b[0;34m(fname, bbox)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mland_ice_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mx_atc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#-----------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# 3) Filter data #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (678564,) (1259,) "
     ]
    }
   ],
   "source": [
    "\n",
    "#!pwd\n",
    "files = list_files_local('../data/RLIS/*ATL03*01.h5')\n",
    "#files = files[0:1]\n",
    "#print(files)\n",
    "\n",
    "njobs = 1#len(files)\n",
    "\n",
    "#NOTE: Using Kamb bounding box for now\n",
    "bbox = None #[-1124782, 81623, -919821, -96334]\n",
    "\n",
    "if njobs == 1:\n",
    "    print('running in serial ...')\n",
    "    [read_atl03(f, bbox) for f in files]\n",
    "\n",
    "else:\n",
    "    print('running in parallel (%d jobs) ...' % njobs)\n",
    "    from joblib import Parallel, delayed\n",
    "    Parallel(n_jobs=njobs, verbose=5)(delayed(read_atl06)(f, bbox) for f in files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131554,) (131554,) (85044,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a472d2483c94997825c99cc528a37fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "def read_h5(fname, vnames=[]):\n",
    "    \"\"\" Simple HDF5 reader. \"\"\"\n",
    "    with h5py.File(fname, 'r') as f:\n",
    "        return [f[v][:] for v in vnames]\n",
    "\n",
    "    \n",
    "files = list_files_local('../data/RLIS/*ATL03*gt*')\n",
    "\n",
    "#33\n",
    "#2\n",
    "#6\n",
    "#18\n",
    "#23\n",
    "#54\n",
    "\n",
    "\n",
    "lon, lat, h,bkg,x_atc = read_h5(files[54], ['lon', 'lat', 'h_elv','bkc_ct','x_atc'])\n",
    "print(lon.shape,h.shape,bkg.shape)\n",
    "\n",
    "plt.plot(h,'.',markersize = 0.1)\n",
    "x_ice, y_ice = transform_coord(4326, 3031, lon, lat)\n",
    " \n",
    "# plt.figure()\n",
    "# plt.plot(h,'.',markersize = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open('data_file1.txt','w')\n",
    "f_out.write('lon\\tlat\\n')\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    f_out.write(str(lat[i]) + '\\t' + str(lon[i]) + '\\n')\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_bands = glob(\"*.TIF\")\n",
    "image1_bands.sort()\n",
    "\n",
    "image1_blue = np.squeeze(rio.open(image1_bands[0]).read())\n",
    "image1_red = np.squeeze(rio.open(image1_bands[2]).read())\n",
    "image1_green = np.squeeze(rio.open(image1_bands[1]).read())\n",
    "\n",
    "image2_blue = np.squeeze(rio.open(image1_bands[3]).read())\n",
    "image2_red = np.squeeze(rio.open(image1_bands[5]).read())\n",
    "image2_green = np.squeeze(rio.open(image1_bands[4]).read())\n",
    "\n",
    "#image1_nir = np.squeeze(rio.open(image1_bands[3]).read())\n",
    "    \n",
    "#Normalize bands into 0.0 - 1.0 scale\n",
    "def normalize(array):\n",
    "    array_min, array_max = array.min(), array.max()\n",
    "    return ((array - array_min)/(array_max - array_min))\n",
    "\n",
    "# Normalize band DN\n",
    "blue = normalize(image1_blue)\n",
    "red = normalize(image1_red)\n",
    "green = normalize(image1_green)\n",
    "\n",
    "blue2 = normalize(image2_blue)\n",
    "red2 = normalize(image2_red)\n",
    "green2 = normalize(image2_green)\n",
    "\n",
    "\n",
    "#ndwi = (green - nir)/(green - nir)\n",
    "# Stack bands\n",
    "rgb = np.dstack((red, green, blue))\n",
    "rgb2 = np.dstack((red2, green2, blue2))\n",
    "#print(nrg.shape())\n",
    "# View the color composite\n",
    "rgb = rgb.astype(float)\n",
    "rgb2 = rgb2.astype(float)\n",
    "#plt.imshow(rgb)\n",
    "#es.plot_bands(image1_blue[0],title=\"Landsat Cropped Band 4\\nColdsprings Fire Scar\",cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9161, 9181, 3)\n"
     ]
    }
   ],
   "source": [
    "print(rgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raster and metadata\n",
    "tile_path = 'LC08_L1GT_167109_20190125_20190205_01_T2_B2.TIF'\n",
    "Raster = gdal.Open(tile_path)\n",
    "width = Raster.RasterXSize\n",
    "height = Raster.RasterYSize\n",
    "gt = Raster.GetGeoTransform()\n",
    "array = Raster.ReadAsArray()\n",
    "\n",
    "# Pixel numbers\n",
    "x = np.arange(0, width)\n",
    "y = np.arange(0, height)\n",
    "\n",
    "# Grid Cell Coordinates of upper left corner in EPSG:3031 UTM. \n",
    "X = gt[0] + x * gt[1] \n",
    "Y = gt[3] + y * gt[5]\n",
    "\n",
    "#xx,yy = np.meshgrid(X,Y)\n",
    "\n",
    "#lon, lat = transform_coord(3031, 4326, xx, yy)\n",
    "\n",
    "# Load raster and metadata\n",
    "tile_path = 'LC08_L1GT_167110_20190125_20190205_01_T2_B2.TIF'\n",
    "Raster = gdal.Open(tile_path)\n",
    "width = Raster.RasterXSize\n",
    "height = Raster.RasterYSize\n",
    "gt = Raster.GetGeoTransform()\n",
    "array = Raster.ReadAsArray()\n",
    "\n",
    "# Pixel numbers\n",
    "x = np.arange(0, width)\n",
    "y = np.arange(0, height)\n",
    "\n",
    "# Grid Cell Coordinates of upper left corner in EPSG:3031 UTM. \n",
    "X2 = gt[0] + x * gt[1] \n",
    "Y2 = gt[3] + y * gt[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493eadfbea2041ffae29ef04048c8008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# # Load \"Natural Earth‚Äù countries dataset, bundled with GeoPandas\n",
    "# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# # Overlay glacier outline\n",
    "# f, ax = plt.subplots(1, figsize=(12, 6))\n",
    "# world.plot(ax=ax, facecolor='lightgray', edgecolor='gray')\n",
    "plt.figure()\n",
    "plt.imshow(rgb, extent = [np.min(X), np.max(X), np.min(Y), np.max(Y)])\n",
    "plt.imshow(rgb2, extent = [np.min(X2), np.max(X2), np.min(Y2), np.max(Y2)])\n",
    "plt.plot(x_ice, y_ice,'r')\n",
    "# ax.set_ylim([-71.5, -67.5])\n",
    "# ax.set_xlim([7.5,15.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
